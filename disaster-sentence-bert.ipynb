{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## SentenceBERT Notebook","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('../input/disaster/train_pp.csv')\ntest_df = pd.read_csv('../input/disaster/test_pp.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = list(train_df['text']) + list(test_df['text'])\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:01:48.668374Z","iopub.execute_input":"2022-05-16T23:01:48.668938Z","iopub.status.idle":"2022-05-16T23:06:05.07118Z","shell.execute_reply.started":"2022-05-16T23:01:48.668888Z","shell.execute_reply":"2022-05-16T23:06:05.06955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:24.832236Z","iopub.execute_input":"2022-05-16T23:24:24.832578Z","iopub.status.idle":"2022-05-16T23:24:25.641176Z","shell.execute_reply.started":"2022-05-16T23:24:24.832543Z","shell.execute_reply":"2022-05-16T23:24:25.640338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = sentence_embeddings[:len(train_df)]\ntest_X = sentence_embeddings[len(train_df):]\ny = np.array(train_df['label'])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:26.109171Z","iopub.execute_input":"2022-05-16T23:24:26.109553Z","iopub.status.idle":"2022-05-16T23:24:26.120962Z","shell.execute_reply.started":"2022-05-16T23:24:26.109516Z","shell.execute_reply":"2022-05-16T23:24:26.119986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ndropout = 0.25","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:43.076943Z","iopub.execute_input":"2022-05-16T23:24:43.07739Z","iopub.status.idle":"2022-05-16T23:24:43.0815Z","shell.execute_reply.started":"2022-05-16T23:24:43.077358Z","shell.execute_reply":"2022-05-16T23:24:43.08078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_X, y, test_size=0.1, random_state=42)\n\nX_tr = torch.tensor(X_train, dtype=torch.float)\ny_tr = torch.tensor(y_train)\ntrain = TensorDataset(X_tr, y_tr)\ntrainloader = DataLoader(train, batch_size=batch_size)\n\nX_te = torch.tensor(X_test, dtype=torch.float)\ny_te = torch.tensor(y_test)\ntest = TensorDataset(X_te, y_te)\ntestloader = DataLoader(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:43.966715Z","iopub.execute_input":"2022-05-16T23:24:43.967009Z","iopub.status.idle":"2022-05-16T23:24:43.986864Z","shell.execute_reply.started":"2022-05-16T23:24:43.966963Z","shell.execute_reply":"2022-05-16T23:24:43.986035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(384, 500)\n        self.hidden1 = nn.Linear(500, 500)\n        self.hidden2 = nn.Linear(500, 500)\n        self.fc2 = nn.Linear(500, 2)\n        self.dropout = nn.Dropout(0.25)\n        \n        self.batchnorm1 = nn.BatchNorm1d(500)\n        self.batchnorm2 = nn.BatchNorm1d(500)\n        self.batchnorm3 = nn.BatchNorm1d(500)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = F.relu(self.hidden1(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = F.relu(self.hidden2(x))\n        x = self.batchnorm3(x)\n        x = self.dropout(x)\n        x = F.log_softmax(self.fc2(x), dim=1)\n        return x\n\nnet = SentenceNet()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:57.552588Z","iopub.execute_input":"2022-05-16T23:24:57.553349Z","iopub.status.idle":"2022-05-16T23:24:57.571068Z","shell.execute_reply.started":"2022-05-16T23:24:57.553313Z","shell.execute_reply":"2022-05-16T23:24:57.570156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.00001","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# create your optimizer\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:24:59.657047Z","iopub.execute_input":"2022-05-16T23:24:59.658114Z","iopub.status.idle":"2022-05-16T23:24:59.663124Z","shell.execute_reply.started":"2022-05-16T23:24:59.658071Z","shell.execute_reply":"2022-05-16T23:24:59.662497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:25:45.17503Z","iopub.execute_input":"2022-05-16T23:25:45.175725Z","iopub.status.idle":"2022-05-16T23:25:45.179708Z","shell.execute_reply.started":"2022-05-16T23:25:45.175684Z","shell.execute_reply":"2022-05-16T23:25:45.178931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_loss = []\nfor epoch in range(num_epochs):\n    net.train()\n    for i, data in enumerate(trainloader):\n        inputs, labels = data\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        training_loss.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 25 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n                   %(epoch+1, num_epochs, i+1, len(trainloader), loss.data))\n    \n    net.eval()\n    outputs = net(X_te)\n\n    _, predicted = torch.max(outputs, 1)\n\n    total = y_te.size(0)\n    correct = (predicted == y_te).sum()\n\n    print(f'Accuracy of the model is: {100*correct/total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:25:45.68636Z","iopub.execute_input":"2022-05-16T23:25:45.686991Z","iopub.status.idle":"2022-05-16T23:26:25.364223Z","shell.execute_reply.started":"2022-05-16T23:25:45.686952Z","shell.execute_reply":"2022-05-16T23:26:25.363333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing\nnet.eval()\noutputs_test = net(test_X)\n\n_, predicted_test = torch.max(outputs_test, 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:28:36.461594Z","iopub.execute_input":"2022-05-16T23:28:36.461891Z","iopub.status.idle":"2022-05-16T23:28:36.550398Z","shell.execute_reply.started":"2022-05-16T23:28:36.461862Z","shell.execute_reply":"2022-05-16T23:28:36.549671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'id': np.array(test_df['id']),\n       'target': np.array(predicted_test)}","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:28:39.116105Z","iopub.execute_input":"2022-05-16T23:28:39.116719Z","iopub.status.idle":"2022-05-16T23:28:39.122903Z","shell.execute_reply.started":"2022-05-16T23:28:39.116674Z","shell.execute_reply":"2022-05-16T23:28:39.1216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.DataFrame(data)\ndf_submission.to_csv('submission_sen_emb.csv', encoding='utf-8', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:28:41.03102Z","iopub.execute_input":"2022-05-16T23:28:41.031657Z","iopub.status.idle":"2022-05-16T23:28:41.062626Z","shell.execute_reply.started":"2022-05-16T23:28:41.031602Z","shell.execute_reply":"2022-05-16T23:28:41.061824Z"},"trusted":true},"execution_count":null,"outputs":[]}]}