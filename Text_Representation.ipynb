{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EQ0UzYj7jJM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from mittens import Mittens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding"
      ],
      "metadata": {
        "id": "8vWkKVs6HShL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sVmIBiu7qNk"
      },
      "outputs": [],
      "source": [
        "def one_hot_dictionary(vocab):\n",
        "  length = len(vocab)\n",
        "  onehotdict = {}\n",
        "  vocab = list(vocab)\n",
        "  for i in range(length):\n",
        "    temp = [0]*length\n",
        "    temp[i] = 1\n",
        "    onehotdict[vocab[i]] = temp\n",
        "  return onehotdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q34x-OL-BNET"
      },
      "outputs": [],
      "source": [
        "ohd = one_hot_dictionary(vocab)\n",
        "f = open(\"one_hot.pkl\",\"wb\")\n",
        "pickle.dump(ohd,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "dfkavvOOHjNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGx6Crfx1YJA",
        "outputId": "473287b1-62a2-4d2f-f70d-65659921cf51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sklearn in /Users/abirami/Library/Python/3.8/lib/python/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/abirami/Library/Python/3.8/lib/python/site-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /Users/abirami/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/abirami/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/abirami/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /Users/abirami/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "def BoW(vocab_sentence):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(vocab_sentence)\n",
        "    df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
        "    d = df_bow_sklearn.to_dict()\n",
        "    bow = {}\n",
        "    for i in df_bow_sklearn:\n",
        "        bow[i] =(list(d[i].values()))\n",
        "    return bow\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvFwE8qKBNEU"
      },
      "outputs": [],
      "source": [
        "b = BoW(vocab_sentence)\n",
        "f = open(\"BoW.pkl\",\"wb\")\n",
        "pickle.dump(b,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF(Text Frequency Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "aan9U-9YHrcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97VRc5TUBNEU"
      },
      "outputs": [],
      "source": [
        "def TFIDF(vocab_sentence):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(vocab_sentence)\n",
        "    df_tfidf_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
        "    d = df_tfidf_sklearn.to_dict()\n",
        "    tfidf = {}\n",
        "    for i in df_tfidf_sklearn:\n",
        "        tfidf[i] = (list(d[i].values()))\n",
        "    return tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65M-0f4JBNEV"
      },
      "outputs": [],
      "source": [
        "t = TFIDF(vocab_sentence)\n",
        "f = open(\"TfIdf.pkl\",\"wb\")\n",
        "pickle.dump(t,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec - Continuous Bag of Words"
      ],
      "metadata": {
        "id": "NHmP47H9IFsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0QojeFbBNEV"
      },
      "outputs": [],
      "source": [
        "def cbow_fun(vocab_sentence):\n",
        "    cbow = Word2Vec([x.split(' ') for x in vocab_sentence], vector_size=100, window=5, min_count=1, workers=4, sg = 10, cbow_mean = 1)\n",
        "    cbow_dict = {}\n",
        "    for i in cbow.wv.key_to_index.keys():\n",
        "        cbow_dict[i] = (cbow.wv[i])\n",
        "    return cbow_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVg5z6hrBNEW"
      },
      "outputs": [],
      "source": [
        "cbow = cbow_fun(vocab_sentence)\n",
        "f = open(\"CBoW.pkl\",\"wb\")\n",
        "pickle.dump(cbow,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec - Skipgram"
      ],
      "metadata": {
        "id": "Uca7rj3fIStx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQH_M91NBNEX"
      },
      "outputs": [],
      "source": [
        "def skipgram_fun(vocab_sentence):\n",
        "    skipgram = Word2Vec([x.split(' ') for x in vocab_sentence], vector_size=100, window=5, min_count=1, workers=4, sg = 1)\n",
        "    sg_dict = {}\n",
        "    for i in skipgram.wv.key_to_index.keys():\n",
        "        sg_dict[i] = (skipgram.wv[i])\n",
        "    return sg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSq7jgk8BNEX"
      },
      "outputs": [],
      "source": [
        "s = skipgram_fun(vocab_sentence)\n",
        "f = open(\"Skipgram.pkl\",\"wb\")\n",
        "pickle.dump(s,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fasttext"
      ],
      "metadata": {
        "id": "aymI690fImsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlFqJ2nBNEX"
      },
      "outputs": [],
      "source": [
        "def fasttext(vocab_sentence):\n",
        "    ft = FastText([x.split(' ') for x in vocab_sentence], vector_size=100, window=5, min_count=1, epochs = 10)\n",
        "    ft_dict = {}\n",
        "    for i in ft.wv.key_to_index.keys():\n",
        "        ft_dict[i] = (ft.wv[i])\n",
        "    return ft_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9w9Um5ABNEX"
      },
      "outputs": [],
      "source": [
        "ft = fasttext(vocab_sentence)\n",
        "f = open(\"fasttext.pkl\",\"wb\")\n",
        "pickle.dump(ft,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glove"
      ],
      "metadata": {
        "id": "__iPgcOFIuPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lxJ_ABpBNEX"
      },
      "outputs": [],
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "glove_path = \"glove.twitter.27B.100d.txt\"\n",
        "pre_glove = glove2dict(glove_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mncOXxgqBNEZ"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "X = cv.fit_transform(doc) #doc has words in the document that does not belong to Glove\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "coocc_ar = Xc.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlbNEUm3BNEZ"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning pre-trained glove\n",
        "mittens_model = Mittens(n=100, max_iter=1000)\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=corp_vocab,\n",
        "    initial_embedding_dict= pre_glove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FdftdetBNEZ"
      },
      "outputs": [],
      "source": [
        "newglove = dict(zip(corp_vocab, new_embeddings))\n",
        "f = open(\"Glove.pkl\",\"wb\")\n",
        "pickle.dump(newglove,f)\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text_Representation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}