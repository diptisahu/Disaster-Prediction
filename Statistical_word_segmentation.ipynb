{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to Mr. Peter Norvig"
      ],
      "metadata": {
        "id": "pVcHF1BBRn-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datafiles\n",
        "from itertools import product\n",
        "from datafiles import datafile\n",
        "from functools import reduce\n",
        "\n",
        "\"\"\"\n",
        "Code to accompany the chapter \"Natural Language Corpus Data\"\n",
        "from the book \"Beautiful Data\" (Segaran and Hammerbacher, 2009)\n",
        "http://oreilly.com/catalog/9780596157111/\n",
        "\n",
        "Code copyright (c) 2008-2009 by Peter Norvig\n",
        "\n",
        "You are free to use this code under the MIT licencse: \n",
        "http://www.opensource.org/licenses/mit-license.php\n",
        "\"\"\"\n",
        "\n",
        "import re, string, random, glob, operator, heapq\n",
        "from collections import defaultdict\n",
        "from math import log10\n",
        "\n",
        "def memo(f):\n",
        "    \"Memoize function f.\"\n",
        "    table = {}\n",
        "    def fmemo(*args):\n",
        "        if args not in table:\n",
        "            table[args] = f(*args)\n",
        "        return table[args]\n",
        "    fmemo.memo = table\n",
        "    return fmemo\n",
        "\n",
        "def test(verbose=None):\n",
        "    \"\"\"Run some tests, taken from the chapter.\n",
        "    Since the hillclimbing algorithm is randomized, some tests may fail.\"\"\"\n",
        "    import doctest\n",
        "    print('Running tests...')\n",
        "    doctest.testfile('ngrams-test.txt', verbose=verbose)\n",
        "\n",
        "################ Word Segmentation (p. 223)\n",
        "\n",
        "@memo\n",
        "def segment(text):\n",
        "    \"Return a list of words that is the best segmentation of text.\"\n",
        "    if not text: return []\n",
        "    candidates = ([first]+segment(rem) for first,rem in splits(text))\n",
        "    return max(candidates, key=Pwords)\n",
        "\n",
        "def splits(text, L=20):\n",
        "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
        "    return [(text[:i+1], text[i+1:]) \n",
        "            for i in range(min(len(text), L))]\n",
        "\n",
        "def Pwords(words): \n",
        "    \"The Naive Bayes probability of a sequence of words.\"\n",
        "    return product(Pw(w) for w in words)\n",
        "\n",
        "#### Support functions (p. 224)\n",
        "\n",
        "def product(nums):\n",
        "    \"Return the product of a sequence of numbers.\"\n",
        "    return reduce(operator.mul, nums, 1)\n",
        "\n",
        "class Pdist(dict):\n",
        "    \"A probability distribution estimated from counts in datafile.\"\n",
        "    def __init__(self, data=[], N=None, missingfn=None):\n",
        "        for key,count in data:\n",
        "            self[key] = self.get(key, 0) + int(count)\n",
        "        self.N = float(N or sum(self.values()))\n",
        "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
        "    def __call__(self, key): \n",
        "        if key in self: return self[key]/self.N  \n",
        "        else: return self.missingfn(key, self.N)\n",
        "\n",
        "def datafile(name, sep='\\t'):\n",
        "    \"Read key,value pairs from file.\"\n",
        "    with open(name, 'r') as file:\n",
        "        for line in file.readlines():\n",
        "            yield line.split(sep)\n",
        "\n",
        "def avoid_long_words(key, N):\n",
        "    \"Estimate the probability of an unknown word.\"\n",
        "    return 10./(N * 10**len(key))\n",
        "\n",
        "N = 1024908267229 ## Number of tokens\n",
        "\n",
        "Pw  = Pdist(datafile('count_1w.txt'), N, avoid_long_words)\n",
        "\n",
        "#### segment2: second version, with bigram counts, (p. 226-227)\n",
        "\n",
        "def cPw(word, prev):\n",
        "    \"Conditional probability of word, given previous word.\"\n",
        "    try:\n",
        "        return P2w[prev + ' ' + word]/float(Pw[prev])\n",
        "    except KeyError:\n",
        "        return Pw(word)\n",
        "\n",
        "P2w = Pdist(datafile('count_2w.txt'), N)\n",
        "\n",
        "@memo \n",
        "def segment2(text, prev='<S>'): \n",
        "    \"Return (log P(words), words), where words is the best segmentation.\" \n",
        "    if not text: return 0.0, [] \n",
        "    candidates = [combine(log10(cPw(first, prev)), first, segment2(rem, first)) \n",
        "                  for first,rem in splits(text)] \n",
        "    return max(candidates) \n",
        "\n",
        "def combine(Pfirst, first, x): \n",
        "    \"Combine first and rem results into one (probability, words) pair.\" \n",
        "    (Prem, rem) = x\n",
        "    return Pfirst+Prem, [first]+rem \n",
        "\n",
        "################ Secret Codes (p. 228-230)\n",
        "\n",
        "def encode(msg, key): \n",
        "    \"Encode a message with a substitution cipher.\" \n",
        "    return msg.translate(string.maketrans(ul(alphabet), ul(key))) \n",
        "\n",
        "def ul(text): return text.upper() + text.lower() \n",
        "\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz' \n",
        "\n",
        "def shift(msg, n=13): \n",
        "    \"Encode a message with a shift (Caesar) cipher.\" \n",
        "    return encode(msg, alphabet[n:]+alphabet[:n]) \n",
        "\n",
        "def logPwords(words): \n",
        "    \"The Naive Bayes probability of a string or sequence of words.\" \n",
        "    if isinstance(words, str): words = allwords(words) \n",
        "    return sum(log10(Pw(w)) for w in words) \n",
        "\n",
        "def allwords(text): \n",
        "    \"Return a list of alphabetic words in text, lowercase.\" \n",
        "    return re.findall('[a-z]+', text.lower()) \n",
        "\n",
        "def decode_shift(msg): \n",
        "    \"Find the best decoding of a message encoded with a shift cipher.\" \n",
        "    candidates = [shift(msg, n) for n in range(len(alphabet))] \n",
        "    return max(candidates, key=logPwords) \n",
        "\n",
        "def shift2(msg, n=13): \n",
        "    \"Encode with a shift (Caesar) cipher, yielding only letters [a-z].\" \n",
        "    return shift(just_letters(msg), n) \n",
        "\n",
        "def just_letters(text): \n",
        "    \"Lowercase text and remove all characters except [a-z].\" \n",
        "    return re.sub('[^a-z]', '', text.lower()) \n",
        "\n",
        "def decode_shift2(msg): \n",
        "    \"Decode a message encoded with a shift cipher, with no spaces.\" \n",
        "    candidates = [segment2(shift(msg, n)) for n in range(len(alphabet))] \n",
        "    p, words = max(candidates) \n",
        "    return ' '.join(words) \n",
        "\n",
        "#### General substitution cipher (p. 231-233)\n",
        "\n",
        "def logP3letters(text): \n",
        "    \"The log-probability of text using a letter 3-gram model.\" \n",
        "    return sum(log10(P3l(g)) for g in ngrams(text, 3)) \n",
        "\n",
        "def ngrams(seq, n):\n",
        "    \"List all the (overlapping) ngrams in a sequence.\"\n",
        "    return [seq[i:i+n] for i in range(1+len(seq)-n)]\n",
        "\n",
        "P3l = Pdist(datafile('count_3l.txt')) \n",
        "P2l = Pdist(datafile('count_2l.txt')) ## We'll need it later \n",
        "\n",
        "def hillclimb(x, f, neighbors, steps=10000): \n",
        "    \"Search for an x that miximizes f(x), considering neighbors(x).\" \n",
        "    fx = f(x) \n",
        "    neighborhood = iter(neighbors(x)) \n",
        "    for i in range(steps): \n",
        "        x2 = neighborhood.next() \n",
        "        fx2 = f(x2) \n",
        "        if fx2 > fx: \n",
        "            x, fx = x2, fx2 \n",
        "            neighborhood = iter(neighbors(x)) \n",
        "    if debugging: print('hillclimb:', x, int(fx))\n",
        "    return x \n",
        "\n",
        "debugging = False \n",
        "\n",
        "def decode_subst(msg, steps=4000, restarts=90): \n",
        "    \"Decode a substitution cipher with random restart hillclimbing.\" \n",
        "    msg = cat(allwords(msg)) \n",
        "    candidates = [hillclimb(encode(msg, key=cat(shuffled(alphabet))), \n",
        "                            logP3letters, neighboring_msgs, steps) \n",
        "                  for _ in range(restarts)] \n",
        "    p, words = max(segment2(c) for c in candidates) \n",
        "    return ' '.join(words) \n",
        "\n",
        "def shuffled(seq): \n",
        "    \"Return a randomly shuffled copy of the input sequence.\" \n",
        "    seq = list(seq) \n",
        "    random.shuffle(seq) \n",
        "    return seq \n",
        "\n",
        "cat = ''.join \n",
        "\n",
        "def neighboring_msgs(msg): \n",
        "    \"Generate nearby keys, hopefully better ones.\" \n",
        "    def swap(a,b): return msg.translate(string.maketrans(a+b, b+a)) \n",
        "    for bigram in heapq.nsmallest(20, set(ngrams(msg, 2)), P2l): \n",
        "        b1,b2 = bigram \n",
        "        for c in alphabet: \n",
        "            if b1==b2: \n",
        "                if P2l(c+c) > P2l(bigram): yield swap(c,b1) \n",
        "            else: \n",
        "                if P2l(c+b2) > P2l(bigram): yield swap(c,b1) \n",
        "                if P2l(b1+c) > P2l(bigram): yield swap(c,b2) \n",
        "    while True: \n",
        "        yield swap(random.choice(alphabet), random.choice(alphabet)) \n",
        "\n",
        "################ Spelling Correction (p. 236-)\n",
        "\n",
        "def corrections(text): \n",
        "    \"Spell-correct all words in text.\" \n",
        "    return re.sub('[a-zA-Z]+', lambda m: correct(m.group(0)), text) \n",
        "\n",
        "def correct(w): \n",
        "    \"Return the word that is the most likely spell correction of w.\" \n",
        "    candidates = edits(w).items() \n",
        "    c, edit = max(candidates, key=lambda x: Pedit(x[1]) * Pw(x[0])) \n",
        "    return c \n",
        "\n",
        "def Pedit(edit): \n",
        "    \"The probability of an edit; can be '' or 'a|b' or 'a|b+c|d'.\" \n",
        "    if edit == '': return (1. - p_spell_error) \n",
        "    return p_spell_error*product(P1edit(e) for e in edit.split('+')) \n",
        "\n",
        "# p_spell_error = 1./20. \n",
        "\n",
        "# P1edit = Pdist(datafile('count_1edit.txt')) ## Probabilities of single edits \n",
        "\n",
        "# def edits(word, d=2): \n",
        "#     \"Return a dict of {correct: edit} pairs within d edits of word.\" \n",
        "#     results = {} \n",
        "#     def editsR(hd, tl, d, edits): \n",
        "#         def ed(L,R): return edits+[R+'|'+L] \n",
        "#         C = hd+tl \n",
        "#         if C in Pw: \n",
        "#             e = '+'.join(edits) \n",
        "#             if C not in results: results[C] = e \n",
        "#             else: results[C] = max(results[C], e, key=Pedit) \n",
        "#         if d <= 0: return \n",
        "#         extensions = [hd+c for c in alphabet if hd+c in PREFIXES] \n",
        "#         p = (hd[-1] if hd else '<') ## previous character \n",
        "#         ## Insertion \n",
        "#         for h in extensions: \n",
        "#             editsR(h, tl, d-1, ed(p+h[-1], p)) \n",
        "#         if not tl: return \n",
        "#         ## Deletion \n",
        "#         editsR(hd, tl[1:], d-1, ed(p, p+tl[0])) \n",
        "#         for h in extensions: \n",
        "#             if h[-1] == tl[0]: ## Match \n",
        "#                 editsR(h, tl[1:], d, edits) \n",
        "#             else: ## Replacement \n",
        "#                 editsR(h, tl[1:], d-1, ed(h[-1], tl[0])) \n",
        "#         ## Transpose \n",
        "#         if len(tl)>=2 and tl[0]!=tl[1] and hd+tl[1] in PREFIXES: \n",
        "#             editsR(hd+tl[1], tl[0]+tl[2:], d-1, \n",
        "#                    ed(tl[1]+tl[0], tl[0:2])) \n",
        "#     ## Body of edits: \n",
        "#     editsR('', word, d, []) \n",
        "#     return results \n",
        "\n",
        "# PREFIXES = set(w[:i] for w in Pw for i in range(len(w) + 1)) \n"
      ],
      "metadata": {
        "id": "n3kbGnD3L40l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Statistical_word_segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}